---
title: RLHF
date: 2024-08-23
categories: ["2024", "LLM"]
tags: ["SFT", "RLHF", "PPO", "DPO", "Rejection sampling", "Reward model"]
use_math: true
---

GPT-3는 다음에 올 토큰을 예측하는 모델이다.  
GPT-3는 어떻게 ChatGPT가 되었는가?

## supervised fine-tuning (SFT)

- 목적 : supervised fine-tuning을 통해 LLM을 사용자의 지시 사항에 맞춰 응답하도록 학습시키고 싶음
- 학습 데이터셋 : instruction dataset (지시사항과 그에 맞는 응답이 있는 데이터셋)
    - "supervised(지도)" fine-tuning은 학습 데이터에 정답이 포함되어 있다.

<img width="40%" alt="supervised fine tuning" src="https://github.com/user-attachments/assets/20c51163-040d-4599-9377-9425eecb4407">
<span style="color:gray; font-size: 80%">출처 : https://aws.amazon.com/ko/what-is/reinforcement-learning-from-human-feedback/</span>

1. 지시 데이터셋 준비
2. 사전학습된 LLM 모델 선택
3. 데이터를 모델이 이해할 수 있는 형식으로 변환
4. 학습률, 배치 크기, 에폭, 손실 함수 등 설정
5. 모델에 데이터 입력하고, 생성된 출력을 기반으로 파라미터 업데이트


```
Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
```
- Alpaca 모델 파인튜닝할 때 사용한 프롬프트 (모델이 이해할 수 있는 형식)
- 프롬프트가 모델이 지시사항을 따르고 맥락을 이해하여 적절한 응답을 생성하도록 설계되었음
- 사전 학습과 동일하게 causal language modeling을 통해 다음 단어 예측

### 좋은 지시 데이터셋이 갖춰야 할 조건
- 지시 데이터셋에 다양한 형태의 지시사항이 존재할 수록 모델의 답변 품질 증가
- 응답 데이터의 품질이 높을 수록 모델의 답변 품질 증가
- 지시 데이터셋의 품질을 높이면 더 적은 데이터셋으로 높은 성능 보일 수 있음
- 교육적 가치가 놓은 데이터셋을 선별하여 사용하면 성능 향상

---------

# Reinforcement Learning from Human Feedback (RLHF)

## 강화 학습을 통한 성능 향상

### reward model 학습

<img width="60%" alt="training a reward model" src="https://github.com/user-attachments/assets/225d4322-c504-4e2f-96b0-571b6d0d375f">
<span style="color:gray; font-size: 80%">출처 : https://aws.amazon.com/ko/what-is/reinforcement-learning-from-human-feedback/</span>

1. 선호 데이터셋 (preference dataset) 구축
    1. supervised fine-tuning을 통하여 모델 학습
    2. 모델이 응답을 여러 개 생성
    3. 응답 간 상대적인 chosen data와 rejected data 선별
2. 선호 데이터셋으로부터 reward model 생성
    1. 모델이 더 높은 점수를 받는 방향으로 응답을 수정하도록 함


### 보상을 통한 학습

<img  alt="스크린샷 2024-09-15 오후 9 02 13" src="https://github.com/user-attachments/assets/461630b5-c53a-4eaf-90eb-cdcc5007e1fc">
<span style="color:gray; font-size: 80%">출처 : https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292</span>

- agent가 environment에서 action을 함
- action에 따라 환경의 state가 바뀌고, 행동에 대한 보상이 생김
- 에이전트는 가능하면 더 많은 보상을 받을 수 있도록 행동을 수정하면서 학습
- 에이전트가 연속적으로 수행하는 행동의 모음 : episode

LLM에서는
- action : 토큰 생성
- episode : 전체 텍스트 모두 생성
- 텍스트를 모두 생성하면 리워드 모델이 텍스트에 점수를 매김 (행동이 아니라 에피소드 단위로 리워드 모델의 점수를 반영함)

### Proximal Preference Optimization (PPO)

<img width="40%" alt="PPO" src="https://github.com/user-attachments/assets/69366276-2d18-4ef4-8ccd-2dfb5a5efce6">
<span style="color:gray; font-size: 80%">출처 : https://aws.amazon.com/ko/what-is/reinforcement-learning-from-human-feedback/</span>


- 리워드 모델을 사용하다 보면 모델이 보상을 높게 받는 데에만 집중하는 reward hacking 이 일어날 수도 있다
- reward hacking 시 성능은 떨어지고 점수만 높게 받는 응답을 생성하게 된다.

PPO (근접 정책 최적화)
- 목적 :
    - 모델의 성능을 향상시키면서도 급격한 policy 변화 방지
    - rewards hacking을 방지하고 안정적인 학습 보장
- 작동 원리 :
    - 현재 policy(파인튜닝 된 모델)와 새로운 polic(학습 중인 모델) 사이의 차이를 제한
    - 파인튜닝 된 모델을 기준으로, 학습하는 모델이 너무 멀지 않은 범위에서 리워드 모델의 높은 점수를 찾도록 함


### RLHF의 한계
- 성능이 높고 robust한 리워드 모델 만들기가 어렵다
- 참고 모델, 학습 모델, 리워드 모델을 사용해야 해서 리소스가 많이 필요하다 (GPU)


------------

## 강화 학습 없이 성능 향상

### rejection sampling

기본 원리 : 여러 개의 응답을 생성하고, 그 중에서 가장 좋은 응답을 선택

1. 파인 튜닝 끝난 모델이 응답 여러 개 생성
2. 리워드 모델이 각 응답에 점수를 부여
3. 가장 높은 점수를 받은 응답 선택
4. 선택된 응답을 모아서 모델 다시 파인튜닝


<img width="100%" alt="스크린샷 2024-09-15 오후 9 17 45" src="https://github.com/user-attachments/assets/7b6ab10d-680a-4c3b-b16f-bed41d6c6c8b">
<span style="color:gray; font-size: 80%">출처 : https://arxiv.org/pdf/2307.09288</span>

LLaMA-2 학습 과정
1. 사전 학습
2. rejection sampling을 사용해서 self-supervised learning (비지도 학습. 다음 단어를 예측하는 언어 모델링 사용)
3. 파인튜닝 $\rightarrow$ LLaMA-2-Chat 모델 생성

- rejection sampling은 PPO 이전 단계에서 사용
- 모델은 사람의 선호를 학습한 상태에서 PPO 학습을 시작할 수 있게 됨

pros)
- 모델의 기본 성능 향상
- 강화 학습 전에 활용하여 후속 강화 학습을 더 안정적으로 만들 수 있음
cons)
- 요구되는 리소스 양이 많을 수도 있음
- 리워드 모델의 품질에 크게 의존


### Direct Preference Optimization (DPO)

기본 원리 : 선호 데이터셋으로 리워드 모델을 만드는 것이 아니라, 선호 데이터셋을 직접 학습시킴

<img width="100%" alt="스크린샷 2024-09-15 오후 9 32 57" src="https://github.com/user-attachments/assets/6e1006b0-8ae2-4173-853b-b1ff8f6ba61b">
<span style="color:gray; font-size: 80%">출처 : https://arxiv.org/pdf/2305.18290</span>

작동 원리 :
- 선호도 데이터셋에는 같은 프롬프트에 대한 두 개의 응답과, 두 응답에 대한 상대적인 선호도가 있음
- 모델은 선호되는 응답의 확률을 높이고, 선호되지 않는 응답의 확률을 낮추는 방향으로 학습
- 학습이 잘 되고 있는지 확인하기 위해 학습 과정에서 참고 모델과 비교
    - 참고 모델은 보통 파인튜닝 마친 모델 사용
    - 학습 중인 모델이 너무 극단적으로 변하는 것을 방지하고, 학습의 안정성 유지

참고 :
- 파인튜닝 된 모델을 상대로 DPO 학습 시켜야 함. 아니면 지시사항과 응답 형식 이해 못 함
- 선호도 데이터셋의 품질이 매우 중요함

pros)
- 리워드 모델링 단계 생략 $\rightarrow$ 학습 과정 단순화
- 계산 효율성 증가
- 안정적인 학습이 가능


-----------
## 출처

<LLM을 활용한 실전 AI 애플리케이션 개발> https://github.com/onlybooks/llm
https://aws.amazon.com/ko/what-is/reinforcement-learning-from-human-feedback/
https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292
https://arxiv.org/pdf/2307.09288
https://arxiv.org/pdf/2305.18290
